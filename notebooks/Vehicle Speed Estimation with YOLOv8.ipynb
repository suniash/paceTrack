{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1BY0TqCZ4_L6Lh-I-5sFXGVWUaDeXLW2v","authorship_tag":"ABX9TyOWB5VSLUfLKqc5gpVdO9i+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Vehicle Speed Estimation with YOLOv8**\n","\n","This Jupyter Notebook demonstrates the application of YOLOv8 for vehicle speed estimation tasks in video sequences. Our goal is to accurately detect vehicles and estimate their speed, which is crucial for a variety of applications ranging from traffic monitoring to automated enforcement of speed limits. Leveraging YOLOv8's state-of-the-art object detection capabilities, we extend its utility to perform not only detection but also to calculate the speed of each identified vehicle within the video frames.\n","\n","This approach capitalizes on YOLOv8's efficient detection mechanism to track vehicle movement across frames and employs mathematical models to estimate their speed based on frame rate and pixel displacement. The notebook navigates through the comprehensive process, encompassing video preprocessing, vehicle detection with YOLOv8, tracking vehicles across frames, speed estimation, and the visualization of results.\n","\n","## **GPU Status Check and Mounting Drive**\n","\n","We begin by checking the availability and status of our GPU, which is crucial for the computationally intensive tasks of video processing and running the YOLOv8 model. The nvidia-smi command gives us a snapshot of the GPU's model, memory usage, and active processes, ensuring our setup is ready for the subsequent operations. Then we mount the google drive."],"metadata":{"id":"BQvCbbnQpb55"}},{"cell_type":"code","source":["!nvidia-smi\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"kdS-yodnSta2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Installing Dependencies**\n","\n","Next, we install necessary Python packages for our project."],"metadata":{"id":"fd1uBx-Gvy0x"}},{"cell_type":"code","source":["!pip install -q supervision ultralytics"],"metadata":{"id":"Tw5nEevlSsmZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Importing Required Libraries**\n","\n","In this section, we import all the necessary libraries and modules required for our vehicle speed estimation project."],"metadata":{"id":"y0EnIAnMv5pG"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"uRApcUx-SMn-","executionInfo":{"status":"ok","timestamp":1709334614675,"user_tz":480,"elapsed":8270,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"}}},"outputs":[],"source":["import cv2\n","import numpy as np\n","import supervision as sv\n","from tqdm import tqdm\n","from ultralytics import YOLO\n","from supervision.assets import VideoAssets, download_assets\n","from collections import defaultdict, deque"]},{"cell_type":"markdown","source":["## **Downloading Video for Vehicle Detection**\n","\n","To facilitate our vehicle speed estimation task, we utilize video that contain various vehicle scenarios from Kaggle."],"metadata":{"id":"bRH0rxEPwHaI"}},{"cell_type":"code","source":["SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/demo_small.mp4\"\n","TARGET_VIDEO_PATH = \"/content/drive/MyDrive/demo_result.mp4\"\n","CONFIDENCE_THRESHOLD = 0.3\n","IOU_THRESHOLD = 0.5\n","MODEL_NAME = \"yolov8x.pt\"\n","MODEL_RESOLUTION = 1280"],"metadata":{"id":"ziOQ4TV1S7HJ","executionInfo":{"status":"ok","timestamp":1709338785964,"user_tz":480,"elapsed":145,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"}}},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":["## **Configuration Parameters Setup**\n","\n","Here, we define key configuration parameters for our vehicle speed estimation project. These parameters are crucial for tuning the performance and output of our vehicle speed estimation process."],"metadata":{"id":"sWbhh5fYwK51"}},{"cell_type":"code","source":["SOURCE = np.array([\n","  [168, 162],\n","  [981, 162],\n","  [1051, 543],\n","  [-228, 543]\n","])\n","TARGET_WIDTH = 25\n","TARGET_HEIGHT = 250\n","TARGET = np.array([ [0, 0], [TARGET_WIDTH - 1, 0], [TARGET_WIDTH - 1, TARGET_HEIGHT - 1], [0, TARGET_HEIGHT - 1], ])"],"metadata":{"id":"JWPwQEpITEpL","executionInfo":{"status":"ok","timestamp":1709338787846,"user_tz":480,"elapsed":133,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"}}},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":["## **Defining Perspective Transformation Parameters**\n","\n","For accurate speed estimation, it's important to account for the perspective distortion in the video footage. This section sets up the parameters for a perspective transformation, aiming to map vehicles' positions from the source video to a standardized view, facilitating consistent speed calculations."],"metadata":{"id":"UalB-9b5wbRm"}},{"cell_type":"code","source":["frame_generator = sv.get_video_frames_generator(source_path=SOURCE_VIDEO_PATH)\n","frame_iterator = iter(frame_generator)\n","frame = next(frame_iterator)"],"metadata":{"id":"0NwZPFtkTKod","executionInfo":{"status":"ok","timestamp":1709338789803,"user_tz":480,"elapsed":667,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"}}},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":["## **Visualizing the Detection Zone**\n","\n","Before proceeding with vehicle detection and speed estimation, it's beneficial to visualize the detection zone within our video frame. This step involves drawing a polygon on the first frame of the video, corresponding to the SOURCE points we defined earlier. The polygon outlines the area of interest on the road where vehicles will be detected and tracked. This visualization aids in verifying that our defined zone accurately represents the portion of the roadway we intend to analyze.\n","\n","The code snippet below copies the first video frame, draws a red polygon to denote the detection zone using the coordinates specified in SOURCE, and then displays the annotated frame. This allows us to visually confirm the detection zone's placement before applying the detection and speed estimation algorithms."],"metadata":{"id":"92mk10QpwlzD"}},{"cell_type":"code","source":["annotated_frame = frame.copy()\n","annotated_frame = sv.draw_polygon(scene=annotated_frame, polygon=SOURCE, color=sv.Color.red(), thickness=4)\n","sv.plot_image(annotated_frame)"],"metadata":{"id":"bIJARhT6TO2t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Implementing Perspective Transformation**\n","\n","To address the challenge of perspective distortion in our vehicle speed estimation task, we define the ViewTransformer class. This class is designed to facilitate the transformation of points from the source perspective to a target perspective, which is critical for accurate speed calculation across different frames."],"metadata":{"id":"Q5f5W_jewv6D"}},{"cell_type":"code","source":["class ViewTransformer:\n","    def __init__(self, source: np.ndarray, target: np.ndarray) -> None:\n","        source = source.astype(np.float32)\n","        target = target.astype(np.float32)\n","        self.m = cv2.getPerspectiveTransform(source, target)\n","\n","    def transform_points(self, points: np.ndarray) -> np.ndarray:\n","        if points.size == 0:\n","            return points\n","        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n","        transform_points = cv2.perspectiveTransform(reshaped_points, self.m)\n","        return transform_points.reshape(-1, 2)\n"],"metadata":{"id":"xLw4785LTVeE","executionInfo":{"status":"ok","timestamp":1709337659101,"user_tz":480,"elapsed":134,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":["## **Initializing the Perspective View Transformer**\n","\n","With the ViewTransformer class defined, we now instantiate an object of this class, view_transformer, using the previously specified SOURCE and TARGET points. This object will be responsible for transforming the coordinates of detected vehicles from the perspective view captured by the camera to a normalized bird's-eye view. This transformation is crucial for accurate speed estimation, as it allows us to measure distances and calculate speeds in a standardized frame of reference, mitigating the effects of perspective distortion.\n","\n","The source parameter is the polygon defined in the video frame where vehicles are detected, and target is the desired polygon in the transformed view, which standardizes the scale for distance measurements. By applying this transformation, we can more accurately estimate the speeds of vehicles as they move across the detection zone."],"metadata":{"id":"op9K1ZwJxIDv"}},{"cell_type":"code","source":["view_transformer = ViewTransformer(source=SOURCE, target=TARGET)"],"metadata":{"id":"Tcymh_kFTvA6","executionInfo":{"status":"ok","timestamp":1709337674912,"user_tz":480,"elapsed":173,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":["## **Vehicle Detection, Tracking, and Speed Estimation**\n","\n","This section outlines the comprehensive process of detecting vehicles, tracking their movements across frames, and estimating their speeds using the YOLOv8 model, combined with custom tracking and annotation tools.\n","\n","### **Model Initialization and Video Preparation**\n","\n","**Model Loading:** Initialize the model.\n","\n","**Video Information:** video_info retrieves details from the source video, such as frame rate and resolution, crucial for processing and annotations.\n","\n","**Frame Generator:** creates an iterable over the video frames, allowing sequential processing.\n","\n","### **Tracking and Annotation Configuration**\n","\n","**ByteTrack Initialization: **An instance of byte_track is created to track vehicles across frames, using the video's frame rate and a confidence threshold.\n","\n","**Annotator Setup:** Bounding_box_annotator, label_annotator, and trace_annotator are configured with dynamic thickness and text scale to visually highlight detected vehicles, their trajectories, and estimated speeds.\n","\n","**Detection Zone:** Polygon_zone defines the area of interest for vehicle detection within the video frames, based on SOURCE coordinates.\n","\n","**Vehicle Detection:** Each frame is processed through the YOLO model to detect vehicles, filtering detections by confidence and class.\n","\n","**Zone Filtering**: Detections outside the predefined polygon zone are excluded.\n","\n","**Non-Max Suppression:** Overlapping detections are refined to ensure each vehicle is tracked individually.\n","\n","**Tracking:** Detected vehicles are tracked across frames, maintaining their identities.\n","\n","**Speed Estimation:** For each tracked vehicle, speed is estimated based on displacement over time, considering the frame rate and transformed coordinates for accuracy.\n","\n","**Annotation:** Each frame is annotated with bounding boxes, labels indicating speed, and traces showing vehicles' paths.\n","\n","**Output:** Annotated frames are compiled into a new video, saved to TARGET_VIDEO_PATH."],"metadata":{"id":"P6V8vsWxxHt2"}},{"cell_type":"code","source":["model = YOLO(MODEL_NAME)\n","video_info = sv.VideoInfo.from_video_path(video_path=SOURCE_VIDEO_PATH)\n","frame_generator = sv.get_video_frames_generator(source_path=SOURCE_VIDEO_PATH)\n","\n","# Tracer initiation\n","byte_track = sv.ByteTrack(frame_rate=video_info.fps, track_thresh=CONFIDENCE_THRESHOLD)\n","\n","# Annotators configuration\n","thickness = sv.calculate_dynamic_line_thickness(resolution_wh=video_info.resolution_wh)\n","text_scale = sv.calculate_dynamic_text_scale(resolution_wh=video_info.resolution_wh)\n","bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=thickness)\n","label_annotator = sv.LabelAnnotator(text_scale=text_scale, text_thickness=thickness, text_position=sv.Position.BOTTOM_CENTER)\n","trace_annotator = sv.TraceAnnotator(thickness=thickness, trace_length=video_info.fps * 2, position=sv.Position.BOTTOM_CENTER)\n","polygon_zone = sv.PolygonZone(polygon=SOURCE, frame_resolution_wh=video_info.resolution_wh)\n","coordinates = defaultdict(lambda: deque(maxlen=video_info.fps))\n","\n","# Open target video\n","with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n","    # Loop over source video frame\n","    for frame in tqdm(frame_generator, total=video_info.total_frames):\n","        result = model(frame, imgsz=MODEL_RESOLUTION, verbose=False)[0]\n","        detections = sv.Detections.from_ultralytics(result)\n","\n","        # Filter out detections by class and confidence\n","        detections = detections[detections.confidence > CONFIDENCE_THRESHOLD]\n","        detections = detections[detections.class_id != 0]\n","\n","        # Filter out detections outside the zone\n","        detections = detections[polygon_zone.trigger(detections)]\n","\n","        # Refine detections using non-max suppression\n","        detections = detections.with_nms(IOU_THRESHOLD)\n","\n","        # Pass detection through the tracker\n","        detections = byte_track.update_with_detections(detections=detections)\n","        points = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n","\n","        # Calculate the detections position inside the target RoI\n","        points = view_transformer.transform_points(points=points).astype(int)\n","\n","        # Store detections position\n","        for tracker_id, [_, y] in zip(detections.tracker_id, points):\n","            coordinates[tracker_id].append(y)\n","\n","        # Format labels\n","        labels = []\n","        for tracker_id in detections.tracker_id:\n","            if len(coordinates[tracker_id]) < video_info.fps / 2:\n","                labels.append(f\"#{tracker_id}\")\n","            else:\n","                # Calculate speed\n","                coordinate_start = coordinates[tracker_id][-1]\n","                coordinate_end = coordinates[tracker_id][0]\n","                distance = abs(coordinate_start - coordinate_end)\n","                time = len(coordinates[tracker_id]) / video_info.fps\n","                speed = distance / time * 3.6\n","                labels.append(f\"#{tracker_id} {int(speed)} km/h\")\n","\n","        # Annotate frame\n","        annotated_frame = frame.copy()\n","        annotated_frame = trace_annotator.annotate(scene=annotated_frame, detections=detections)\n","        annotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=detections)\n","        annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n","\n","        # Add frame to target video\n","        sink.write_frame(annotated_frame)\n"],"metadata":{"id":"Q8V51oeETy4K"},"execution_count":null,"outputs":[]}]}