{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","mount_file_id":"1ktBrywfZb183bbVhxLP5MZKq31SVb6Dy","authorship_tag":"ABX9TyMtQIeIDEuQSBRdT+LkFU+Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Track and Count Vehicles with YOLOv8**\n","\n","This Jupyter Notebook is dedicated to tracking and counting vehicles in video footage using advanced object detection models. The primary aim is to accurately identify, follow, and enumerate different types of vehicles across video frames. This analysis is pivotal for traffic flow management, urban planning, and automated surveillance systems. Leveraging deep learning models, specifically designed for object detection in videos, this notebook outlines a comprehensive approach for real-time vehicle tracking and counting.\n","\n","## **GPU Status Check**\n","\n","We begin by checking the availability and status of our GPU, which is crucial for the computationally intensive tasks of video processing and running the YOLOv8 model. The nvidia-smi command gives us a snapshot of the GPU's model, memory usage, and active processes, ensuring our setup is ready for the subsequent operations."],"metadata":{"id":"i7nhgjnF0UFq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YgqxDHxzorgn"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","source":["## **Importing Libraries and Setting Up the Workspace**\n","\n","After downloading the necessary video file for vehicle tracking and counting, this cell focuses on importing essential Python libraries and modules that will be used throughout the notebook. Additionally, it reaffirms the home directory setup, ensuring all file paths are correctly managed."],"metadata":{"id":"9oLyalJE0gZa"}},{"cell_type":"code","source":["import os\n","import sys\n","from typing import List\n","\n","import numpy as np\n","from IPython.display import display\n","from tqdm.notebook import tqdm\n","\n","HOME = os.getcwd()\n","print(HOME)"],"metadata":{"id":"TJDaYZhepHH-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Downloading Video for Analysis**\n","\n","Ensure the working directory is set to HOME and download the vehicle-counting.mp4 video using a Google Drive link. This step is critical for making sure the necessary video file is present for our vehicle tracking and counting tasks."],"metadata":{"id":"qsHQWVkg3kBH"}},{"cell_type":"code","source":["%cd {HOME}\n","!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1pz68D1Gsx80MoPg-_q-IbEdESEmyVLm-' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1pz68D1Gsx80MoPg-_q-IbEdESEmyVLm-\" -O vehicle-counting.mp4 && rm -rf /tmp/cookies.txt"],"metadata":{"id":"xUM-DILDpLFs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Setting the Source Video Path**\n","\n","Define the path to the video file used for vehicle tracking and counting analysis, ensuring it's correctly located within the notebook's home directory for easy access during processing."],"metadata":{"id":"Tt21rLNs3nPF"}},{"cell_type":"code","source":["SOURCE_VIDEO_PATH = f\"{HOME}/vehicle-counting.mp4\""],"metadata":{"id":"ipBdfxzlpNnU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## **Installing Ultralytics and Preparing the Environment**\n","\n","Install the ultralytics package, clear the output to maintain a clean notebook, and verify the installation by checking the environment setup."],"metadata":{"id":"KhDdayTs3yDV"}},{"cell_type":"code","source":["!pip install ultralytics\n","\n","display.clear_output()\n","\n","import ultralytics\n","ultralytics.checks()"],"metadata":{"id":"JnDTAlBXpQYN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## **Setting Up ByteTrack for Enhanced Vehicle Tracking**\n","\n","This section is dedicated to installing ByteTrack, a high-performance vehicle tracking system, and preparing it for integration with our vehicle detection and counting project"],"metadata":{"id":"GDcLiT9t39Ht"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KdBkOflo2xY"},"outputs":[],"source":["%cd {HOME}\n","!git clone https://github.com/ifzhang/ByteTrack.git\n","%cd {HOME}/ByteTrack\n","\n","!sed -i 's/onnx==1.8.1/onnx==1.9.0/g' requirements.txt\n","\n","!pip3 install -q -r requirements.txt\n","!python3 setup.py -q develop\n","!pip install -q cython_bbox\n","!pip install -q onemetric\n","\n","!pip install -q loguru lap thop\n","\n","display.clear_output()\n","\n","sys.path.append(f\"{HOME}/ByteTrack\")\n","\n","import yolox\n","print(\"yolox.__version__:\", yolox.__version__)"]},{"cell_type":"markdown","source":["## **Integrating BYTETracker for Vehicle Tracking**\n","\n","This section integrates BYTETracker from the ByteTrack framework into our vehicle tracking and counting project. BYTETracker is renowned for its high-performance multi-object tracking capabilities, making it ideal for tracking vehicles in video streams.\n","\n"],"metadata":{"id":"eZ8msUHA4KN8"}},{"cell_type":"code","source":["from yolox.tracker.byte_tracker import BYTETracker, STrack\n","from onemetric.cv.utils.iou import box_iou_batch\n","from dataclasses import dataclass\n","\n","@dataclass(frozen=True)\n","class BYTETrackerArgs:\n","    track_thresh: float = 0.25\n","    track_buffer: int = 30\n","    match_thresh: float = 0.8\n","    aspect_ratio_thresh: float = 3.0\n","    min_box_area: float = 1.0\n","    mot20: bool = False"],"metadata":{"id":"bmtxgCNnpTdr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Installing a Specific Version of the Supervision Library**\n","\n","To ensure compatibility and access to specific features required for vehicle tracking and counting, this step involves installing version 0.1.0 of the supervision library. Following the installation, the output is cleared to maintain a clean notebook workspace, and the installed version of supervision is verified.\n","\n"],"metadata":{"id":"6Wp9EynS4dGt"}},{"cell_type":"code","source":["!pip install supervision==0.1.0\n","\n","display.clear_output()\n","\n","import supervision\n","print(\"supervision.__version__:\", supervision.__version__)"],"metadata":{"id":"v4oNuuyapW2t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Importing Supervision Tools for Video Processing and Annotation**\n","\n","This step involves importing various modules and classes from the supervision library that are essential for handling video data, drawing annotations, and processing detections in our vehicle tracking and counting project.\n","\n"],"metadata":{"id":"jial9-m14ho4"}},{"cell_type":"code","source":["from supervision.draw.color import ColorPalette\n","from supervision.geometry.dataclasses import Point\n","from supervision.video.dataclasses import VideoInfo\n","from supervision.video.source import get_video_frames_generator\n","from supervision.video.sink import VideoSink\n","from supervision.notebook.utils import show_frame_in_notebook\n","from supervision.tools.detections import Detections, BoxAnnotator\n","from supervision.tools.line_counter import LineCounter, LineCounterAnnotator"],"metadata":{"id":"qpoPLfqnpZgv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## **Utility Functions for Processing Detections and Tracks**\n","\n","This section introduces utility functions designed to facilitate the handling and association of detection results with tracked objects in our vehicle tracking and counting project. These functions convert detection and track information into a format suitable for further processing and matching."],"metadata":{"id":"HNUsVknz4umN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SE0G6LvFAXlk"},"outputs":[],"source":["def detections2boxes(detections: Detections) -> np.ndarray:\n","    return np.hstack((\n","        detections.xyxy,\n","        detections.confidence[:, np.newaxis]\n","    ))\n","\n","def tracks2boxes(tracks: List[STrack]) -> np.ndarray:\n","    return np.array([\n","        track.tlbr\n","        for track\n","        in tracks\n","    ], dtype=float)\n","\n","# matches our bounding boxes with predictions\n","def match_detections_with_tracks(\n","    detections: Detections,\n","    tracks: List[STrack]\n",") -> Detections:\n","    if not np.any(detections.xyxy) or len(tracks) == 0:\n","        return np.empty((0,))\n","\n","    tracks_boxes = tracks2boxes(tracks=tracks)\n","    iou = box_iou_batch(tracks_boxes, detections.xyxy)\n","    track2detection = np.argmax(iou, axis=1)\n","\n","    tracker_ids = [None] * len(detections)\n","\n","    for tracker_index, detection_index in enumerate(track2detection):\n","        if iou[tracker_index, detection_index] != 0:\n","            tracker_ids[detection_index] = tracks[tracker_index].track_id\n","\n","    return tracker_ids"]},{"cell_type":"markdown","source":["\n","## **Specifying the YOLOv8 Model for Vehicle Detection**\n","\n","This line of code sets the model to be used for vehicle detection to YOLOv8x, indicating the specific version of the YOLO model optimized for accuracy and performance in detecting objects within video frames."],"metadata":{"id":"Ci0PflAG4xfx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"m3FMq5FcUsRc"},"outputs":[],"source":["MODEL = \"yolov8x.pt\""]},{"cell_type":"markdown","source":["\n","## **Loading and Optimizing the YOLOv8x Model for Vehicle Detection**\n","\n","This section involves importing the YOLO class from the ultralytics package, initializing the YOLO model with the specified model configuration, and applying model optimization techniques to enhance performance."],"metadata":{"id":"daqATJeS5FF5"}},{"cell_type":"code","source":["from ultralytics import YOLO\n","model = YOLO(MODEL)\n","model.fuse()"],"metadata":{"id":"60T12yaVpcHY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Configuring Class IDs for Vehicle Detection**\n","\n","This step involves setting up a dictionary to map class IDs to their corresponding class names for the YOLOv8x model and specifying the class IDs of interest for vehicle detection, such as cars, motorcycles, buses, and trucks."],"metadata":{"id":"ZoBuNoID4-1M"}},{"cell_type":"code","source":["# dict maping class_id to class_name\n","CLASS_NAMES_DICT = model.model.names\n","# class_ids of interest - car, motorcycle, bus and truck\n","CLASS_ID = [2, 3, 5, 7]"],"metadata":{"id":"cJe7PERzperv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Initializing Video Processing and Detection**\n","\n","This section outlines the steps to generate frames from the video, annotate detected vehicles, and display the processed frame. It involves setting up a frame generator, initializing a BoxAnnotator for drawing detections, performing a model prediction on the first frame, and visually presenting the results."],"metadata":{"id":"3CdnrjIQ5Z9v"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZuNPZ2hvHeZV"},"outputs":[],"source":["# create frame generator\n","generator = get_video_frames_generator(SOURCE_VIDEO_PATH)\n","# create instance of BoxAnnotator\n","box_annotator = BoxAnnotator(color=ColorPalette(), thickness=4, text_thickness=4, text_scale=2)\n","# acquire first video frame\n","iterator = iter(generator)\n","frame = next(iterator)\n","# model prediction on single frame and conversion to supervision Detections\n","results = model(frame)\n","detections = Detections(\n","    xyxy=results[0].boxes.xyxy.cpu().numpy(),\n","    confidence=results[0].boxes.conf.cpu().numpy(),\n","    class_id=results[0].boxes.cls.cpu().numpy().astype(int)\n",")\n","# format custom labels\n","labels = [\n","    f\"{CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n","    for _, confidence, class_id, tracker_id\n","    in detections\n","]\n","# annotate and display frame\n","frame = box_annotator.annotate(frame=frame, detections=detections, labels=labels)\n","\n","%matplotlib inline\n","show_frame_in_notebook(frame, (16, 16))"]},{"cell_type":"markdown","source":["## **Configuring Line Detection and Output Video Path**\n","\n","This step specifies settings for a line used in vehicle counting and defines the path for saving the processed video with annotations.\n","\n"],"metadata":{"id":"FIAPu8ld5ZVs"}},{"cell_type":"code","source":["# settings\n","LINE_START = Point(50, 1500)\n","LINE_END = Point(3840-50, 1500)\n","\n","TARGET_VIDEO_PATH = f\"{HOME}/vehicle-counting-result.mp4\""],"metadata":{"id":"-Trq4LphpnVT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Extracting Video Information**\n","\n","This step involves obtaining essential information about the source video, such as frame rate, resolution, and total frames. This information serves as the basis for subsequent video processing steps."],"metadata":{"id":"aAMhKHKL6pqG"}},{"cell_type":"code","source":["VideoInfo.from_video_path(SOURCE_VIDEO_PATH)"],"metadata":{"id":"PQzEuKRSpsIO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Implementing Vehicle Tracking and Counting**\n","\n","This section combines vehicle detection, tracking, and counting into a single workflow, utilizing BYTETracker for tracking, annotations for visual feedback, and a line counter for vehicle counting.\n","\n","**Initialize Tracking and Annotation Tools:** bold text Set up BYTETracker, video metadata retrieval, frame generation, line counting, and annotations for boxes and lines.\n","\n","**Process Video Frames:**\n","- Detect vehicles using YOLO and filter detections by class.\n","- Update BYTETracker with detections for tracking.\n","- Match detections to tracks and update tracker IDs.\n","- Count vehicles crossing a predefined line.\n","Annotate frames with detection boxes and line crossings.\n","\n","**Output:** Write annotated frames to a target video file."],"metadata":{"id":"zdLEEb7j58oK"}},{"cell_type":"code","source":["# create BYTETracker instance\n","byte_tracker = BYTETracker(BYTETrackerArgs())\n","# create VideoInfo instance\n","video_info = VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n","# create frame generator\n","generator = get_video_frames_generator(SOURCE_VIDEO_PATH)\n","# create LineCounter instance\n","line_counter = LineCounter(start=LINE_START, end=LINE_END)\n","# create instance of BoxAnnotator and LineCounterAnnotator\n","box_annotator = BoxAnnotator(color=ColorPalette(), thickness=4, text_thickness=4, text_scale=2)\n","line_annotator = LineCounterAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","\n","# open target video file\n","with VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n","    # loop over video frames\n","    for frame in tqdm(generator, total=video_info.total_frames):\n","        # model prediction on single frame and conversion to supervision Detections\n","        results = model(frame)\n","        detections = Detections(\n","            xyxy=results[0].boxes.xyxy.cpu().numpy(),\n","            confidence=results[0].boxes.conf.cpu().numpy(),\n","            class_id=results[0].boxes.cls.cpu().numpy().astype(int)\n","        )\n","        # filtering out detections with unwanted classes\n","        mask = np.array([class_id in CLASS_ID for class_id in detections.class_id], dtype=bool)\n","        detections.filter(mask=mask, inplace=True)\n","        # tracking detections\n","        tracks = byte_tracker.update(\n","            output_results=detections2boxes(detections=detections),\n","            img_info=frame.shape,\n","            img_size=frame.shape\n","        )\n","        tracker_id = match_detections_with_tracks(detections=detections, tracks=tracks)\n","        detections.tracker_id = np.array(tracker_id)\n","        # filtering out detections without trackers\n","        mask = np.array([tracker_id is not None for tracker_id in detections.tracker_id], dtype=bool)\n","        detections.filter(mask=mask, inplace=True)\n","        # format custom labels\n","        labels = [\n","            f\"#{tracker_id} {CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n","            for _, confidence, class_id, tracker_id\n","            in detections\n","        ]\n","        # updating line counter\n","        line_counter.update(detections=detections)\n","        # annotate and display frame\n","        frame = box_annotator.annotate(frame=frame, detections=detections, labels=labels)\n","        line_annotator.annotate(frame=frame, line_counter=line_counter)\n","        sink.write_frame(frame)"],"metadata":{"id":"6Qaq3cCQCgXd"},"execution_count":null,"outputs":[]}]}