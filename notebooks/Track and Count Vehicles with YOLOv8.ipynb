{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","mount_file_id":"1ktBrywfZb183bbVhxLP5MZKq31SVb6Dy","authorship_tag":"ABX9TyOvu+bHa0TPqVk5u34KoUF0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Track and Count Vehicles with YOLOv8**\n","\n","This Jupyter Notebook is dedicated to tracking and counting vehicles in video footage using advanced object detection models. The primary aim is to accurately identify, follow, and enumerate different types of vehicles across video frames. This analysis is pivotal for traffic flow management, urban planning, and automated surveillance systems. Leveraging deep learning models, specifically designed for object detection in videos, this notebook outlines a comprehensive approach for real-time vehicle tracking and counting.\n","\n","## **GPU Status Check**\n","\n","We begin by checking the availability and status of our GPU, which is crucial for the computationally intensive tasks of video processing and running the YOLOv8 model. The nvidia-smi command gives us a snapshot of the GPU's model, memory usage, and active processes, ensuring our setup is ready for the subsequent operations."],"metadata":{"id":"i7nhgjnF0UFq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YgqxDHxzorgn"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","source":["## **Importing Libraries and Setting Up the Workspace**\n","\n","After downloading the necessary video file for vehicle tracking and counting, this cell focuses on importing essential Python libraries and modules that will be used throughout the notebook. Additionally, it reaffirms the home directory setup, ensuring all file paths are correctly managed."],"metadata":{"id":"9oLyalJE0gZa"}},{"cell_type":"code","source":["import os\n","import sys\n","from typing import List\n","\n","import numpy as np\n","from IPython.display import display, clear_output\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","\n","HOME = os.getcwd()\n","print(HOME)"],"metadata":{"id":"TJDaYZhepHH-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Mounting the Drive**\n","\n"],"metadata":{"id":"qsHQWVkg3kBH"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"YmPP-cGSBmzS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls \"/content/drive/My Drive/\""],"metadata":{"id":"y8127c18B0IH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Setting the Source Video Path**\n","\n","Define the path to the video file used for vehicle tracking and counting analysis, ensuring it's correctly located within the notebook's home directory for easy access during processing."],"metadata":{"id":"Tt21rLNs3nPF"}},{"cell_type":"code","source":["%cd {HOME}\n","SOURCE_VIDEO_PATH = f\"{HOME}/drive/MyDrive/demo_trim.mp4\""],"metadata":{"id":"ipBdfxzlpNnU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## **Installing Ultralytics and Preparing the Environment**\n","\n","Install the ultralytics package, clear the output to maintain a clean notebook, and verify the installation by checking the environment setup."],"metadata":{"id":"KhDdayTs3yDV"}},{"cell_type":"code","source":["!pip install ultralytics\n","\n","from IPython import display\n","display.clear_output()\n","\n","import ultralytics\n","ultralytics.checks()\n"],"metadata":{"id":"JnDTAlBXpQYN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Installing a Specific Version of the Supervision Library**\n","\n","To ensure compatibility and access to specific features required for vehicle tracking and counting, this step involves installing version 0.1.0 of the supervision library. Following the installation, the output is cleared to maintain a clean notebook workspace, and the installed version of supervision is verified.\n","\n"],"metadata":{"id":"6Wp9EynS4dGt"}},{"cell_type":"code","source":["!pip install supervision==0.13.0\n","\n","\n","from IPython import display\n","display.clear_output()\n","\n","\n","import supervision\n","print(\"supervision.__version__:\", supervision.__version__)"],"metadata":{"id":"v4oNuuyapW2t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709245442140,"user_tz":480,"elapsed":6671,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"}},"outputId":"c9666e3e-a48b-4265-da24-8473a1e83699"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["supervision.__version__: 0.13.0\n"]}]},{"cell_type":"markdown","source":["\n","## **Specifying the YOLOv8 Model for Vehicle Detection**\n","\n","This line of code sets the model to be used for vehicle detection to YOLOv8x, indicating the specific version of the YOLO model optimized for accuracy and performance in detecting objects within video frames."],"metadata":{"id":"Ci0PflAG4xfx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"m3FMq5FcUsRc"},"outputs":[],"source":["MODEL = \"yolov8x.pt\""]},{"cell_type":"markdown","source":["\n","## **Loading and Optimizing the YOLOv8x Model for Vehicle Detection**\n","\n","This section involves importing the YOLO class from the ultralytics package, initializing the YOLO model with the specified model configuration, and applying model optimization techniques to enhance performance."],"metadata":{"id":"daqATJeS5FF5"}},{"cell_type":"code","source":["from ultralytics import YOLO\n","model = YOLO(MODEL)\n","model.fuse()"],"metadata":{"id":"60T12yaVpcHY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Configuring Class IDs for Vehicle Detection**\n","\n","This step involves setting up a dictionary to map class IDs to their corresponding class names for the YOLOv8x model and specifying the class IDs of interest for vehicle detection, such as cars, motorcycles, buses, and trucks."],"metadata":{"id":"ZoBuNoID4-1M"}},{"cell_type":"code","source":["# dict maping class_id to class_name\n","CLASS_NAMES_DICT = model.model.names\n","\n","# class_ids of interest - car, motorcycle, bus and truck\n","selected_classes = [2, 3, 5, 7]"],"metadata":{"id":"cJe7PERzperv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Initializing Video Processing and Detection**\n","\n","This section outlines the steps to generate frames from the video, annotate detected vehicles, and display the processed frame. It involves setting up a frame generator, initializing a BoxAnnotator for drawing detections, performing a model prediction on the first frame, and visually presenting the results."],"metadata":{"id":"3CdnrjIQ5Z9v"}},{"cell_type":"code","source":["import supervision as sv\n","import numpy as np"],"metadata":{"id":"uBx9WNwjNh4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create frame generator\n","generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n","# create instance of BoxAnnotator\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","# acquire first video frame\n","iterator = iter(generator)\n","frame = next(iterator)\n","# model prediction on single frame and conversion to supervision Detections\n","results = model(frame)[0]\n","\n","# convert to Detections\n","detections = sv.Detections.from_ultralytics(results)\n","# only consider class id from selected_classes define above\n","detections = detections[np.isin(detections.class_id, selected_classes)]\n","\n","# format custom labels\n","labels = [\n","    f\"{CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n","    for _, _, confidence, class_id, _ in detections\n","]\n","\n","# annotate and display frame\n","anotated_frame=box_annotator.annotate(scene=frame, detections=detections, labels=labels)\n","\n","%matplotlib inline\n","sv.plot_image(anotated_frame, (16,16))"],"metadata":{"id":"bUxWX3PbNiKR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Configuring Line Detection and Output Video Path**\n","\n","This step specifies settings for a line used in vehicle counting and defines the path for saving the processed video with annotations.\n","\n"],"metadata":{"id":"FIAPu8ld5ZVs"}},{"cell_type":"code","source":["# settings\n","LINE_START = sv.Point(50, 1500)\n","LINE_END = sv.Point(3840-50, 1500)\n","\n","TARGET_VIDEO_PATH = f\"{HOME}/drive/MyDrive/demo_vehicle_result.mp4\""],"metadata":{"id":"-Trq4LphpnVT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Extracting Video Information**\n","\n","This step involves obtaining essential information about the source video, such as frame rate, resolution, and total frames. This information serves as the basis for subsequent video processing steps."],"metadata":{"id":"aAMhKHKL6pqG"}},{"cell_type":"code","source":["sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)"],"metadata":{"id":"PQzEuKRSpsIO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709245448760,"user_tz":480,"elapsed":37,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"}},"outputId":"6f941e15-9359-4e0a-8751-3c47ebe6ea01"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VideoInfo(width=1280, height=720, fps=59, total_frames=1208)"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["## **Implementing Vehicle Tracking and Counting**\n","\n","This section combines vehicle detection, tracking, and counting into a single workflow, utilizing BYTETracker for tracking, annotations for visual feedback, and a line counter for vehicle counting.\n","\n","**Initialize Tracking and Annotation Tools:** bold text Set up BYTETracker, video metadata retrieval, frame generation, line counting, and annotations for boxes and lines.\n","\n","**Process Video Frames:**\n","- Detect vehicles using YOLO and filter detections by class.\n","- Update BYTETracker with detections for tracking.\n","- Match detections to tracks and update tracker IDs.\n","- Count vehicles crossing a predefined line.\n","Annotate frames with detection boxes and line crossings.\n","\n","**Output:** Write annotated frames to a target video file."],"metadata":{"id":"zdLEEb7j58oK"}},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","\n","# create BYTETracker instance\n","byte_tracker = sv.ByteTrack(track_thresh= 0.25, track_buffer = 30,match_thresh = 0.8,frame_rate =30)\n","\n","# create VideoInfo instance\n","video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n","\n","# create frame generator\n","generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n","\n","# create LineZone instance, it is previously called LineCounter class\n","line_zone = sv.LineZone(start=LINE_START, end=LINE_END)\n","\n","# create instance of BoxAnnotator\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","\n","# create LineZoneAnnotator instance, it is previously called LineCounterAnnotator class\n","line_zone_annotator = sv.LineZoneAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","\n","progress_bar = tqdm(total=video_info.total_frames, desc=\"Processing Video\")\n","\n","# define call back function to be used in video processing\n","def callback(frame: np.ndarray, index:int) -> np.ndarray:\n","  global process_bar\n","  # model prediction on single frame and conversion to supervision Detections\n","  results = model(frame)[0]\n","  detections = sv.Detections.from_ultralytics(results)\n","  # only consider class id from selected_classes define above\n","  detections = detections[np.isin(detections.class_id, selected_classes)]\n","  # tracking detections\n","  detections = byte_tracker.update_with_detections(detections)\n","  labels = [\n","     f\"#{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\"\n","     for _, _, confidence, class_id, tracker_id\n","     in detections\n","  ]\n","  # update progress bar\n","  progress_bar.update(1)\n","\n","  box_annotated_frame=box_annotator.annotate(scene=frame.copy(),\n","                                detections=detections,\n","                                labels=labels)\n","  # update line counter\n","  line_zone.trigger(detections)\n","  # return frame with box and line annotated result\n","  return  line_zone_annotator.annotate(box_annotated_frame, line_counter=line_zone)\n","\n","# process the whole video\n","sv.process_video(\n","    source_path = SOURCE_VIDEO_PATH,\n","    target_path = TARGET_VIDEO_PATH,\n","    callback=callback\n",")\n","\n","progress_bar.close()"],"metadata":{"id":"6Qaq3cCQCgXd"},"execution_count":null,"outputs":[]}]}